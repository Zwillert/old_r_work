---
title: "HW 6"
author: "Zach Willert"
output: html_document
---



# Part 1:


## Number 1:


### A)


The 'less wrong' blog is "an online community dedicated to the study of human rationality, the general field of improving our decisions" (can't describe the theme better than that!). The audience is pretty broad: a multidisciplinary coalition of folks from various sciences, maths and philosophy -- anyone interested in viewing the world through a mathematically objectivist viewpoint (although demographically pretty narrow, for example, 82.8% of users are cisgender men). The blog was started by Eliezer Yudkowsky, who remains a major contributor, however, most of the content is generated by various users of the site.


### B)
Timeline:

* 1700's: Thomas Bayes first conceived of the iterative process of learning which is now the foundation of Bayesian statistics: Initial Belief + New Data = Improved Belief. These ideas were first published after Bayes's death in 1761, found by a friend looking through his notebooks. 


* 1774: Pierre-Simon Laplace independently discovered and published Bayes's ideas. 

* 1781: Laplace discovers Bayes's work, emboldening a fledgling career of applied and theoretical Bayesian work. 

* 1800's: Initially popular, Bayes's theorem gradually draws criticism from mathematicians and philosohpers. 
* 1925: Mathematician R.A. Fisher publishes significant frequentist work, marking the significant decline of Bayesian statistics of the past 50 years. 

* 1939: Harold Jeffreys publishes 'Theory of Probability', a major contribution to Bayesian theory and a practical guide for scientific use of Bayesian statistics. 

* 1940's: Alan Turing uses Bayesian methods to crack German encoded messages. The allies then win the war. 

* 1950's: Although Bayesian statistics was still widely criticised and unused, some statisticians write significant works (laying the ground for the Bayesian revolution). The revival had begun. 

* 1960: First presidential election is 'called' by statistical methods (before the full vote count was in). 

* 1980's: Various high profile uses of Bayes's theorem return its popularity and credibility. 

* 1990's: Technological progress, the invention of the Markov Chain Monte Carlo method kicks off the 'Bayesian revolution' in most statistical academic disciplines. 




### C)

* My favorite use in the Laplace era (though this was a less direct application) was to provide an epitstemological basis for secularism. As the blog says: "Napoleon asked why Laplace had failed to mention God in his book on the subject. Laplace replied: 'Sire, I have no need of that hypothesis.'"

* Come on. Obviously the best ever use of Bayes was to beat the Nazi's. 

* The most interesting application in this time period for me was "the Bible-quoting business executive Arthur Bailey"

* The most interesting application, as well as my favorite (and least favorite) from the 'practical use' period is the prediction of presidential elections, and the formalization of Bayesian methods in the modern political campaign. 

## Number 2:

### A)  

At https://projects.fivethirtyeight.com/2016-election-forecast/ :

I am interested in politics. Infact, it was my attempt at modeling and predicting the Republican primaries that got me interested in math in the first place! 

This isn't exactly an 'article', but it is certaintly one of the most famous representations of Bayesian methods so far. This web page details the predictions for the 2016 American Presidential elections, as generated by the Bayesian models built by Nate Silver at fivethirtyeight. This webpage communicates important findings from these models with a variety of infographics. These visuals include a state by state victory prediction map, and a time series graph of the predicted probability of overall electoral college victory for both candidates. The page also includes probability estimates for trival concerns, such as the distribution of the popular vote. Most importantly, this web page gives hints about the structure and methodology of the models, and provides a link to read an in depth description of how the forecast works. 

N. (2016, November 08). 2016 Election Forecast. Retrieved March 04, 2017, from https://projects.fivethirtyeight.com/2016-election-forecast/


### B)

Machine Learning of Jazz Grammars:

Machine Learning of Jazz Grammars is an excellent paper describing the application of machine learning techniques in understanding and reproducing jazz. The first step in this project is to define 'grammars' of jazz, or a set of metrics and variables which classify the various aspects of music. Informed by the literature, the paper details and justifies a set of measurements which form the grammar. These researchers employ markov chains to describe and work with musical phrases: "sets of abstract melodies serve as the states in the markov chain. Given a starting melody, we add the next phrase based on a list of transition probabilities from the first measure", and a k-means clustering algorithm to group these abstract melodies. The paper finds that there is a measurable difference of style between different jazz artists, and it documents a blind evaluation of their stylistic reproductions wherein 85% of test subjects were able to correctly identify all 3 styles which this method modeled and reproduced. Neat!

Gillick, J., Tang, K., & Keller, R. M. (2010). Machine Learning of Jazz Grammars. Computer Music Journal, 34(3), 56-66. doi:10.1162/comj_a_00006


# Activity 8 

## Getting Started


```{r}
running.plot = function(x,bars=FALSE,...){
 n = length(x)

 #Calculate the running mean of x:
 run.mean = cumsum(x)/c(1:n)
 
 #Calculate the running margin of error
 moe.run = rep(0,n-1)
 for(i in 2:(n-1)){
  moe.run[i-1] = 1.96*sd(x[1:i])/sqrt(i)
 }
 
 if(bars=="FALSE"){
  #Plot the running mean versus sample size:
  plot(c(1:n), run.mean, type="l", ...) 
 }
  if(bars=="TRUE"){
  #Plot the running mean versus sample size:
  plot(c(1:n), run.mean, type="l", ylim=c(min(run.mean[-1]-moe.run),max(run.mean[-1]+moe.run)), ...) 
  lines(c(2:n), run.mean[-1] + moe.run, col=2)
  lines(c(2:n), run.mean[-1] - moe.run, col=2)
 }
}


PAvotes <- read.csv("https://www.macalester.edu/~ajohns24/data/VoterFraud.csv")
NEWvotes <- PAvotes[-22,]
```




<br>
<br>


## PART 1


### EXERCISE 1


Quantity               $E(\mu|Z)$   $P(\mu < -2|Z)$   95% CI for $\mu$
--------------------- ------------ ----------------- -------------------
Truth                 -5.805905     0.9930498         (-8.810884, -2.770288)
Monte Carlo estimate  -5.805102     0.9936331         (-8.800304, -2.811506)  
`rjags` estimate      -5.838335     0.9999999         (-8.771615, -2.905055)  


<br>
<br>

### EXERCISE 2

**part a.**
```{r}


meanNV = mean(NEWvotes$z)

```


**part b.** 

```{r}

simple_posterior = c(((0.42) * meanNV)/(0.42+(1/100)),(0.42+(1/900))^(-1))

simple_posterior
```




**part c**


pnorm(-2, -5.790586, sd= sqrt(2.37467))
qnorm(0.025, -5.790586, sd= sqrt(2.37467))
qnorm(0.975, -5.790586, sd= sqrt(2.37467))



<br>
<br>

### EXERCISE 3

```{r}
#set the random number seed for reproducibility & consistent results
set.seed(1989)
MCsample <- rnorm(1000, mean=-5.790586, sd=sqrt(2.37467))
#check out the first 50 
MCsample[1:50]
```


<br>
<br>

### EXERCISE 4

**part a.** 

```{r}
plot(MCsample[1:10] ~ c(1:10), xlab="draw number", ylab=expression(paste("sampled ", mu)), type="l")
plot(MCsample ~ c(1:1000), xlab="draw number", ylab=expression(paste("sampled ", mu)), type="l")
```


No noticable patterns here! Looking pretty random... which is what we would expect from our sampling method. 


**part b.** 


```{r}
hist(MCsample, freq=FALSE, xlab=expression(paste("sampled ", mu, " values")))
suppressPackageStartupMessages(library(mosaic))
densityplot(MCsample, xlab=expression(paste("sampled ", mu, " values")))  
```


**part c.** 

Comment: 

Without a doubt this works pretty well! 

Plot: 

```{r}
hist(MCsample, freq=FALSE, xlab=expression(paste("sampled ", mu, " values")))
curve(dnorm(x, mean=-5.790586, sd=sqrt(2.37467)), -15, 5, add=T, col=2)
```





<br>
<br>



## Exercise 5

**part a.** 

```{r}
mean(MCsample)
pnorm(-2, mean(MCsample), sd(MCsample))
qnorm(0.025, mean(MCsample), sd(MCsample))
qnorm(0.975, mean(MCsample), sd(MCsample))

```


**part b.** 

These estimates are pretty aligned, and they have convinced me that Monte Carlo can indeed work. 

<br>
<br>

## Exercise 6

Code: 

```{r}
par(mfrow=c(1,2))
running.plot(MCsample, bars=FALSE, xlab="sample size", ylab="estimate")
running.plot(MCsample, bars=TRUE, xlab="sample size", ylab="estimate")
abline(h=-5.790586, col=2, lty=2)
par(mfrow=c(1,1))
```



**part a.** 

This is some law of large numbers action at play: the smaller the sample size, the more variance we can expect in the sample. But the larger the sample becomes, the more it will converge to the true parameters. 


**part b.** 

The narrowing of the confidence bands means that we can be more confident about Monte Carlo estimates of higher sample size. 



<br>
<br>

## EXERCISE 7


```{r}
library(rjags)

#specify the model
VoteModel1 = "model{
    #Data
    for(i in 1:length(z)) {
        #Note: dnorm in rjags takes the precision, not st. dev.
        z[i] ~ dnorm(mu, 0.02)
    }
    #Prior
    mu ~ dnorm(0, 1/900)
}"
```


<br>
<br>

## EXERCISE 8


<br>
<br>

## EXERCISE 9


<br>
<br>


## EXERCISE 10  

Code:

**part a.** 



**part b.** 





<br>
<br>

## EXERCISE 11


Comment:  

Plot: 
    



<br>
<br>


## EXERCISE 12

source          $\hat{\mu}$   95% CI for $\mu$    $\hat{\tau}$   95% CI for $\tau$
------------- -------------- ------------------- -------------- -------------------
prior                
frequentist       
posterior         



<br>
<br>

## EXERCISE 13


    
<br>
<br>

## EXERCISE 14

**part a.**


**part b.** 



<br>
<br>

## EXERCISE 15

    

<br>
<br>

## EXERCISE 16




<br>
<br>

## EXERCISE 17



<br>
<br>

## EXERCISE 18

**part a.** 


**part b.** 


**part c.** 





<br>
<br>

## EXERCISE 19



<br>
<br>

## EXERCISE 20

**part a.**




**part b.**




**part c.** 




<br>
<br>

## EXERCISE 21





<br>
<br>

## EXERCISE 22

Code:

**part a.**




**part b.**



**part c.**






